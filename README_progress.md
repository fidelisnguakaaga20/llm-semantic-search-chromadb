# âœ… PROGRESS SO FAR (LLM ROADMAP)

ðŸ”¥ MONTH 1 â€” Python + Transformers + Embeddings

**WEEK 1: Python â€“ DONE**

- Basic syntax: variables, lists, dicts
- Functions and modules
- File handling (`02_files.py` writing/reading `study_plan.txt`)
- Jupyter basics in `01_python_basics.ipynb`

**WEEK 2: HuggingFace Basics â€“ DONE**

- Loaded GPT-2 with `AutoTokenizer` + `AutoModelForCausalLM`
- Tokenized text and inspected `input_ids` + tokens
- Generated text with `pipeline("text-generation", model="gpt2")`
- Extracted embeddings by:
  - taking `outputs.hidden_states[-1]`
  - mean-pooling over sequence to get sentence embeddings

**WEEK 3: Embeddings + Vector Search â€“ DONE**

- Sentence Transformers model: `sentence-transformers/all-MiniLM-L6-v2`
- Local Chroma DB (`chroma_db/`) with collection for study sentences
- Semantic search script: `03_chroma_search.py`
  - CLI: type a query â†’ embed â†’ Chroma â†’ see most similar sentences
- Notebook version: `03_embeddings_search.ipynb`
- Pushed as **Project 1 â€“ Semantic Search with SentenceTransformers + ChromaDB**
  - Repo: `llm-semantic-search-chromadb`

**WEEK 4: Transformer Concepts â€“ DONE**

- Script: `04_transformer_concepts.py`
- Demonstrated:
  - Tokenization (subwords, IDs)
  - Attention shapes and weights (last layer, specific head)
  - High-level architecture of GPT-style decoder blocks (embeddings, masked self-attention, MLP, residuals, LayerNorm)

---

ðŸ”¥ MONTH 2 â€” RAG + LangChain + Backend

**WEEK 5: RAG â€“ PDF â†’ Chunks â†’ Embeddings â†’ Chroma â€“ DONE**

- Resume PDF stored at `data/sample.pdf`
- Script: `data/05_rag_pdf_basic.py`
- Steps:
  - Load PDF with `pypdf.PdfReader`
  - Chunk text into overlapping windows
  - Embed chunks with `SentenceTransformer("all-MiniLM-L6-v2")`
  - Store in Chroma collection (`pdf_rag_chunks`)
  - CLI Q&A: type a question â†’ retrieve top-k chunks â†’ return best chunk as answer

**WEEK 6: LangChain RAG â€“ DONE (Local CLI)**

- Script: `06_langchain_resume_rag.py`
- Uses **LangChain**:
  - `PyPDFLoader` to load `data/sample.pdf`
  - `RecursiveCharacterTextSplitter` for chunking
  - `HuggingFaceEmbeddings` (`all-MiniLM-L6-v2`)
  - `Chroma` vector store (`chroma_db_langchain_resume`)
- RAG loop:
  - You type a question in the terminal
  - It retrieves the most relevant chunks
  - Uses a **local GPT-2** pipeline to generate a noisy answer
  - Shows both the retrieved context and final answer

**WEEK 7: FastAPI Backend â€“ IN PROGRESS (Core Endpoints DONE)**

- Script: `07_fastapi_resume_api.py`
- Tech:
  - FastAPI + Uvicorn
  - Chroma (same resume chunks)
  - Sentence Transformers (`all-MiniLM-L6-v2`)
- Implemented endpoints:
  - `GET /health` â€“ health check
  - `POST /embed` â€“ embed arbitrary text
  - `POST /search` â€“ semantic search over resume chunks
  - `POST /rag` â€“ simple RAG:
    - retrieve top-k chunks
    - return best chunk as `answer` plus context list
- Tested via FastAPI docs at `http://127.0.0.1:8000/docs`:
  - `/search` â†’ returns top chunks + distances
  - `/rag` â†’ returns `answer` (best chunk) + `context`
- This is **Project 2 â€“ Resume RAG API (LangChain + FastAPI + Chroma)**

Next: finish Week 7 (optional `/chat`, `/agent` stubs) and move to **Week 8 â€“ Next.js frontend integration**.


<!-- # LLM Roadmap â€“ Progress Log

This file tracks my progress against the MASTER LLM ENGINEERING ROADMAP.

- Month 1 â€“ Week 1: Python âœ…
- Month 1 â€“ Week 2: HuggingFace Basics âœ…
- Month 1 â€“ Week 3: Embeddings + Vector Search âœ…
- Month 1 â€“ Week 4: Transformer Concepts âœ…
- Month 2 â€“ Week 5: RAG (PDF â†’ chunks â†’ embeddings â†’ Chroma) âœ…
- Month 2 â€“ Week 6: LangChain RAG over my resume âœ…
- Month 2 â€“ Week 7: FastAPI Resume RAG API ðŸš§ (in progress)


# âœ… PROGRESS SO FAR (UP TO MONTH 2, WEEK 6)

## Month 1 â€” Python + Transformers + Embeddings

### WEEK 1: Python âœ…
Covered exactly as planned:

- **Variables / types / printing**
  - `01_basics.py`
- **Lists & dicts**
  - Create, update, index, loop over them
- **Functions**
  - Simple functions + parameters, return values
- **File handling**
  - `02_files.py` â†’ writes & reads `study_plan.txt`
- **Jupyter Notebook**
  - Opened notebook, ran cells, printed outputs

> Status: âœ… Week 1 fully complete.

---

### WEEK 2: HuggingFace Basics âœ…
All 4 bullets done using **GPT-2** locally:

- **Load model**
  - `test_llm.py` + `02_hf_basics.ipynb`  
  - `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline("text-generation", model="gpt2")`
- **Tokenize text**
  - Inspected `input_ids`, tokens, and decoded text in notebook
- **Generate text**
  - Generated continuations for prompts like  
    `"Learning LLMs with confidence:"`
- **Extract embeddings**
  - Took `hidden_states[-1]` from the model and mean-pooled to get a sentence embedding

> Status: âœ… Week 2 fully complete.

---

### WEEK 3: Embeddings + Vector Search âœ…
Used **SentenceTransformers + ChromaDB** and built a real mini-project.

- **Use Sentence Transformers**
  - Model: `sentence-transformers/all-MiniLM-L6-v2`
- **Store embeddings in Chroma**
  - Created local `chroma_db` and a collection
- **Query by similarity**
  - `collection.query(...)` with `query_embeddings`
- **Build simple search engine**
  - `03_chroma_search.py` â†’ interactive CLI:
    - Enter query â†’ encode â†’ search in Chroma â†’ show most similar sentences
  - `03_embeddings_search.ipynb` â†’ same logic in notebook
- **Project saved to GitHub**
  - Project: **Semantic Search with SentenceTransformers + Chroma**
  - Repo: `llm-semantic-search-chromadb`

> Status: âœ… Week 3 fully complete and already portfolio-ready (Project 1).

---

### WEEK 4: Transformers Concepts âœ…
Focused on **understanding GPT-style transformers** in code.

- **Tokenization (deeper)**
  - `04_transformer_concepts.py`
  - Showed how a sentence turns into sub-tokens + IDs
- **Attention (basic)**
  - Enabled model attentions and printed:
    - attention shape: `(batch, heads, seq_len, seq_len)`
    - which tokens the last token attends to most
- **Decoder models (GPT-style)**
  - Explained:
    - input embeddings + positional embeddings
    - masked self-attention (no look-ahead)
    - feed-forward layers
    - residuals + layer norm
    - stacked decoder blocks â†’ logits for next token

> Status: âœ… Week 4 fully complete (good mental model of how GPT-like models work).

---

## Month 2 â€” RAG + LangChain + Backend

### WEEK 5: RAG (Manual PDF â†’ Chunks â†’ Embeddings â†’ Chroma â†’ Answer) âœ…
Built a **manual RAG pipeline** over your own resume PDF.

- **PDF loader**
  - `05_rag_pdf_basic.py` using `pypdf.PdfReader`
  - Reads `data/sample.pdf` (your resume)
- **Chunking**
  - Simple character-based chunks with overlap
- **Embedding chunks**
  - SentenceTransformers: `all-MiniLM-L6-v2`
- **Vector DB indexing**
  - Stores chunk embeddings into **Chroma** collection `pdf_rag_chunks`
- **Retrieval + answer**
  - For each question:
    - Embed question â†’ query Chroma â†’ get top chunks
    - Print best chunk as **â€œANSWER FROM PDF (BEST CHUNK)â€**

Examples you ran:

- â€œWhat are my top projects?â€
- â€œWhat tech stack do I use?â€
- â€œWhat is my phone number?â€

> Status: âœ… Week 5 complete.  
> You now have a working **resume RAG system** without LangChain.

---

### WEEK 6: LangChain / RAG Pipeline (FIRST PART DONE) âœ…
Rebuilt the resume RAG pipeline using **LangChain** on top of the same ideas.

- **Chains / RAG pipeline (core retrieval + LLM)** âœ…
  - File: `06_langchain_resume_rag.py`
  - Uses:
    - `PyPDFLoader` to load `data/sample.pdf`
    - `RecursiveCharacterTextSplitter` for chunking
    - `HuggingFaceEmbeddings` (`all-MiniLM-L6-v2`)
    - `Chroma` vectorstore (`chroma_db_langchain_resume`)
    - `.as_retriever()` to get relevant chunks
  - For each CLI question:
    - Prints **Top retrieved chunks**
    - Uses local `gpt2` via `pipeline("text-generation")` to generate an answer
    - Note: `gpt2` is tiny, so answers are messy â€” this is expected for now.  
      The important part is **LangChain + vectorstore + retrieval** are working.

Questions you tested:

- â€œWhat are my top projects?â€
- â€œWhat tech stack do I use?â€
- â€œWhere can recruiters see my live projects?â€

> Status: âœ… RAG pipeline with LangChain is working over your resume.  
> Remaining Week 6 topics (tools, memory, more advanced agents) will be layered on top later.

---

## Current Portfolio Projects From This Roadmap

1. **Project 1 â€“ Semantic Search with SentenceTransformers + Chroma (CLI)**
   - Stack: Python, `sentence-transformers`, `chromadb`
   - Features: encodes sentences, stores in Chroma, CLI semantic search.
   - Repo: `llm-semantic-search-chromadb`

2. **Project 2 â€“ Resume Q&A RAG (Manual + LangChain, CLI)**
   - Stack: Python, `pypdf`, `sentence-transformers`, `chromadb`, `langchain-community`, `langchain`, local `gpt2`
   - Features:
     - Load your PDF resume
     - Chunk â†’ embed â†’ store in Chroma
     - Ask questions; retrieve best chunks about your skills/projects/stack
     - LangChain version wraps the same logic into a retriever + simple chain

You are now **fully on track up to:**

- âœ… Month 1: Weeks 1â€“4  
- âœ… Month 2: Week 5 and the **core RAG pipeline part of Week 6**

Next steps (still following the roadmap):

- Finish the rest of **Week 6** (tools, memory, simple agents on top of this RAG).
- Then move into **Week 7: FastAPI backend** (`/embed`, `/search`, `/rag`, `/chat`, `/agent`). -->
